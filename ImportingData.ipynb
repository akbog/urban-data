{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import json\n",
    "import os\n",
    "import gzip\n",
    "import shutil\n",
    "import fnmatch\n",
    "import sklearn\n",
    "\n",
    "from zipfile import ZipFile\n",
    "\n",
    "from six import string_types\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "from nltk.corpus.reader.util import StreamBackedCorpusView, concat, ZipFilePathPointer\n",
    "\n",
    "from nltk.corpus.reader.api import CorpusReader\n",
    "\n",
    "from nltk.corpus.reader.twitter import TwitterCorpusReader\n",
    "\n",
    "from nltk.data import gzip_open_unicode\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "#Documentation for nltk's twitter corpus reader\n",
    "#https://www.nltk.org/_modules/nltk/corpus/reader/twitter.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These functions exist to unzip gzip files and zip them into regular .zip \n",
    "#files, s.t. nltk's TwitterCorpusReader can interpret\n",
    "def gunzip(file_path,output_path):\n",
    "    with gzip.open(file_path,\"rb\") as f_in, open(output_path,\"wb\") as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)\n",
    "    os.remove(output_path + \".gz\")\n",
    "\n",
    "def recurse_and_gunzip(root):\n",
    "    walker = os.walk(root)\n",
    "    for root,dirs,files in walker:\n",
    "        print(files)\n",
    "        for f in files:\n",
    "            if fnmatch.fnmatch(f,\"*.gz\"):\n",
    "                gunzip('Twitter-Data/Data/{}'.format(f),'Twitter-Data/Data/{}'.format(f.replace(\".gz\",\"\")))\n",
    "\n",
    "def recurse_and_zip(root):\n",
    "    walker = os.walk(root)\n",
    "    for root, dirs, files in walker:\n",
    "        for f in files:\n",
    "            if fnmatch.fnmatch(f, \"*.json\"):\n",
    "                with ZipFile('{}.zip'.format(f), mode = \"w\") as zf:\n",
    "                    zf.write('Twitter-Data/Data/{}'.format(f))\n",
    "                os.remove(f)\n",
    "\n",
    "# recurse_and_gunzip(root)\n",
    "# recurse_and_zip(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOC_PATTERN = r'\\bManhattan\\b'\n",
    "# CAT_PATTERN = r'^(19|20)\\d\\d[- /.](0[1-9]|[012])[-/.](0[1-9]|[12][0-9]|3[01])$'\n",
    "corpus = TwitterCorpusReader('Twitter-Data/Data', \"demo4.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'nltk.corpus.reader.util.StreamBackedCorpusView'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'extended_entities': {'media': [{'display_url': 'pic.twitter.com/z2vX7SF9OT',\n",
       "    'indices': [223, 246],\n",
       "    'sizes': {'small': {'w': 680, 'h': 397, 'resize': 'fit'},\n",
       "     'large': {'w': 1241, 'h': 725, 'resize': 'fit'},\n",
       "     'thumb': {'w': 150, 'h': 150, 'resize': 'crop'},\n",
       "     'medium': {'w': 1200, 'h': 701, 'resize': 'fit'}},\n",
       "    'id_str': '1192385043689345025',\n",
       "    'expanded_url': 'https://twitter.com/LawFirmBusiness/status/1192385046801502208/photo/1',\n",
       "    'media_url_https': 'https://pbs.twimg.com/media/EIwz4TSXkAEA7b0.jpg',\n",
       "    'id': 1192385043689345025,\n",
       "    'type': 'photo',\n",
       "    'media_url': 'http://pbs.twimg.com/media/EIwz4TSXkAEA7b0.jpg',\n",
       "    'url': 'https://t.co/z2vX7SF9OT'}]},\n",
       " 'entities': {'urls': [],\n",
       "  'hashtags': [{'indices': [200, 209], 'text': 'SLFS2019'},\n",
       "   {'indices': [210, 222], 'text': 'ProfitFirst'}],\n",
       "  'media': [{'display_url': 'pic.twitter.com/z2vX7SF9OT',\n",
       "    'indices': [223, 246],\n",
       "    'sizes': {'small': {'w': 680, 'h': 397, 'resize': 'fit'},\n",
       "     'large': {'w': 1241, 'h': 725, 'resize': 'fit'},\n",
       "     'thumb': {'w': 150, 'h': 150, 'resize': 'crop'},\n",
       "     'medium': {'w': 1200, 'h': 701, 'resize': 'fit'}},\n",
       "    'id_str': '1192385043689345025',\n",
       "    'expanded_url': 'https://twitter.com/LawFirmBusiness/status/1192385046801502208/photo/1',\n",
       "    'media_url_https': 'https://pbs.twimg.com/media/EIwz4TSXkAEA7b0.jpg',\n",
       "    'id': 1192385043689345025,\n",
       "    'type': 'photo',\n",
       "    'media_url': 'http://pbs.twimg.com/media/EIwz4TSXkAEA7b0.jpg',\n",
       "    'url': 'https://t.co/z2vX7SF9OT'}],\n",
       "  'user_mentions': [],\n",
       "  'symbols': []},\n",
       " 'full_text': 'It’s today!  Super excited to be there, and presenting this morning at 10am about how to have a more profitable law firm.  It’s not always easy... but it’s worth it! Look forward to seeing you there. #SLFS2019 #ProfitFirst https://t.co/z2vX7SF9OT',\n",
       " 'display_text_range': [0, 222]}"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# corpus.docs()[4]['extended_tweet']\n",
    "print(type(corpus.docs()))\n",
    "corpus.docs()[4]['extended_tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from six import string_types, text_type\n",
    "\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.internals import slice_bounds\n",
    "from nltk.data import PathPointer, FileSystemPathPointer, ZipFilePathPointer\n",
    "from nltk.data import SeekableUnicodeStreamReader\n",
    "from nltk.util import AbstractLazySequence, LazySubsequence, LazyConcatenation, py25\n",
    "import struct\n",
    "import io\n",
    "\n",
    "class GzipStreamBackedCorpusView(StreamBackedCorpusView):\n",
    "    \n",
    "    def __init__(self, fileid, block_reader = None, startpos = 0, encoding = 'utf8'):\n",
    "        StreamBackedCorpusView.__init__(self, fileid, block_reader=block_reader, startpos=0, encoding='utf8')\n",
    "        \n",
    "        try:\n",
    "            if isinstance(self._fileid, PathPointer):\n",
    "                \n",
    "                self._eofpos = self.getuncompressedsize(self._fileid)\n",
    "            else:\n",
    "                self._eofpos = os.stat(self._fileid).st_size\n",
    "        except Exception as exc:\n",
    "            raise ValueError('Unable to open or access %r -- %s' % (fileid, exc))\n",
    "\n",
    "    def _open(self):\n",
    "        if isinstance(self._fileid, PathPointer):\n",
    "#             self._stream = self._fileid.open(self._encoding)\n",
    "            self._stream = gzip.open(str(self._fileid), 'rb')\n",
    "#             print(self._stream)\n",
    "        elif self._encoding:\n",
    "            print(\"here1\")\n",
    "            self._stream = SeekableUnicodeStreamReader(\n",
    "                gzip.open(self._fileid, 'rb'), self._encoding\n",
    "            )\n",
    "        else:\n",
    "            print(\"here2\")\n",
    "            self._stream = open(self._fileid, 'rb')\n",
    "    \n",
    "    def getuncompressedsize(self, filename):\n",
    "        with gzip.GzipFile(filename, 'r') as fin:\n",
    "            fin.seek(-4,2)\n",
    "#             val = struct.unpack('I', fin.read(4))[0]\n",
    "            val = fin.seek(0, io.SEEK_END)\n",
    "            print(val)\n",
    "            return val\n",
    "    \n",
    "    def iterate_from(self, start_tok):\n",
    "        # Start by feeding from the cache, if possible.\n",
    "        if self._cache[0] <= start_tok < self._cache[1]:\n",
    "            for tok in self._cache[2][start_tok - self._cache[0] :]:\n",
    "                yield tok\n",
    "                start_tok += 1\n",
    "\n",
    "        # Decide where in the file we should start.  If `start` is in\n",
    "        # our mapping, then we can jump straight to the correct block;\n",
    "        # otherwise, start at the last block we've processed.\n",
    "        if start_tok < self._toknum[-1]:\n",
    "            block_index = bisect.bisect_right(self._toknum, start_tok) - 1\n",
    "            toknum = self._toknum[block_index]\n",
    "            filepos = self._filepos[block_index]\n",
    "        else:\n",
    "            block_index = len(self._toknum) - 1\n",
    "            toknum = self._toknum[-1]\n",
    "            filepos = self._filepos[-1]\n",
    "\n",
    "        # Open the stream, if it's not open already.\n",
    "        if self._stream is None:\n",
    "            self._open()\n",
    "\n",
    "        # If the file is empty, the while loop will never run.\n",
    "        # This *seems* to be all the state we need to set:\n",
    "        if self._eofpos == 0:\n",
    "            self._len = 0\n",
    "\n",
    "        # Each iteration through this loop, we read a single block\n",
    "        # from the stream.\n",
    "        while filepos < self._eofpos:\n",
    "            # Read the next block.\n",
    "            self._stream.seek(filepos)\n",
    "            self._current_toknum = toknum\n",
    "            self._current_blocknum = block_index\n",
    "            tokens = self.read_block(self._stream)\n",
    "            assert isinstance(tokens, (tuple, list, AbstractLazySequence)), (\n",
    "                'block reader %s() should return list or tuple.'\n",
    "                % self.read_block.__name__\n",
    "            )\n",
    "            num_toks = len(tokens)\n",
    "            new_filepos = self._stream.tell()\n",
    "            if new_filepos <= filepos:\n",
    "                print(new_filepos, filepos)\n",
    "            assert new_filepos > filepos, (\n",
    "                'block reader %s() should consume at least 1 byte (filepos=%d)'\n",
    "                % (self.read_block.__name__, filepos)\n",
    "            )\n",
    "\n",
    "            # Update our cache.\n",
    "            self._cache = (toknum, toknum + num_toks, list(tokens))\n",
    "\n",
    "            # Update our mapping.\n",
    "            assert toknum <= self._toknum[-1]\n",
    "            if num_toks > 0:\n",
    "                block_index += 1\n",
    "                if toknum == self._toknum[-1]:\n",
    "                    assert new_filepos > self._filepos[-1]  # monotonic!\n",
    "                    self._filepos.append(new_filepos)\n",
    "                    self._toknum.append(toknum + num_toks)\n",
    "                else:\n",
    "                    # Check for consistency:\n",
    "                    assert (\n",
    "                        new_filepos == self._filepos[block_index]\n",
    "                    ), 'inconsistent block reader (num chars read)'\n",
    "                    assert (\n",
    "                        toknum + num_toks == self._toknum[block_index]\n",
    "                    ), 'inconsistent block reader (num tokens returned)'\n",
    "\n",
    "            # If we reached the end of the file, then update self._len\n",
    "            if new_filepos == self._eofpos:\n",
    "                self._len = toknum + num_toks\n",
    "            # Generate the tokens in this block (but skip any tokens\n",
    "            # before start_tok).  Note that between yields, our state\n",
    "            # may be modified.\n",
    "            for tok in tokens[max(0, start_tok - toknum) :]:\n",
    "                yield tok\n",
    "            # If we're at the end of the file, then we're done.\n",
    "            assert new_filepos <= self._eofpos\n",
    "            if new_filepos == self._eofpos:\n",
    "                break\n",
    "            # Update our indices\n",
    "            toknum += num_toks\n",
    "            filepos = new_filepos\n",
    "\n",
    "        # If we reach this point, then we should know our length.\n",
    "        assert self._len is not None\n",
    "        # Enforce closing of stream once we reached end of file\n",
    "        # We should have reached EOF once we're out of the while loop.\n",
    "        self.close()\n",
    "\n",
    "class NewTwitterCorpusReader(TwitterCorpusReader):\n",
    "    \n",
    "    CorpusView = GzipStreamBackedCorpusView\n",
    "    \n",
    "    def __init__(\n",
    "        self, root, fileids=None, word_tokenizer=TweetTokenizer(), encoding='utf8'\n",
    "    ):\n",
    "        TwitterCorpusReader.__init__(self, root, fileids)\n",
    "    \n",
    "    def docs(self, fileids = None):\n",
    "        #Concat is a method to create a ConcattenatedStreamBackedCorpusView\n",
    "        return concat(\n",
    "            [\n",
    "                self.CorpusView(path, block_reader = self._read_tweets, encoding=enc)\n",
    "                for (path, enc, fileid) in self.abspaths(fileids, True, True)\n",
    "            ]\n",
    "        )\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = NewTwitterCorpusReader('Twitter-Data/2019-11-07', ['Manhattan-2019-11-07-000.json.gz'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153871498\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "40706"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus.docs())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solved - The issue has to do with the following process:\n",
    "1. When TwitterCorpusReader uses the \"docs\" method, it relies on the abspaths function of the parent CorpusReader class, which initializes PathPointer Objects, not taking into consideration whether a specific file is zipped\n",
    "2. This is because the TwitterCorpusReader is not expecting an individually zipped file but rather is expecting either unzipped files, or zipped by the root\n",
    "3. This makes sense, as it is not relying on a CategorizedCorpusReader, which would differentiate based on the Root Object - this is important to take into consideration\n",
    "4. Step 4 is to take into consideration CategorizedCorpusReader objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from six import string_types, text_type\n",
    "\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.internals import slice_bounds\n",
    "from nltk.data import PathPointer, FileSystemPathPointer, ZipFilePathPointer, GzipFileSystemPathPointer\n",
    "from nltk.data import SeekableUnicodeStreamReader\n",
    "from nltk.util import AbstractLazySequence, LazySubsequence, LazyConcatenation, py25\n",
    "import struct\n",
    "import io\n",
    "import re\n",
    "\n",
    "class GzipStreamBackedCorpusView(StreamBackedCorpusView):\n",
    "    \n",
    "    def __init__(self, fileid, block_reader = None, startpos = 0, encoding = 'utf8'):\n",
    "        StreamBackedCorpusView.__init__(self, fileid, block_reader=block_reader, startpos=0, encoding='utf8')\n",
    "        \n",
    "        try:\n",
    "            if isinstance(self._fileid, PathPointer):\n",
    "                \n",
    "                self._eofpos = self.getuncompressedsize(self._fileid)\n",
    "            else:\n",
    "                self._eofpos = os.stat(self._fileid).st_size\n",
    "        except Exception as exc:\n",
    "            raise ValueError('Unable to open or access %r -- %s' % (fileid, exc))\n",
    "\n",
    "    def _open(self):\n",
    "        if isinstance(self._fileid, PathPointer):\n",
    "#             self._stream = self._fileid.open(self._encoding)\n",
    "            self._stream = gzip.open(str(self._fileid), 'rb')\n",
    "#             print(self._stream)\n",
    "        elif self._encoding:\n",
    "            print(\"here1\")\n",
    "            self._stream = SeekableUnicodeStreamReader(\n",
    "                gzip.open(self._fileid, 'rb'), self._encoding\n",
    "            )\n",
    "        else:\n",
    "            print(\"here2\")\n",
    "            self._stream = open(self._fileid, 'rb')\n",
    "    \n",
    "    def getuncompressedsize(self, filename):\n",
    "        with gzip.GzipFile(filename, 'r') as fin:\n",
    "            fin.seek(-4,2)\n",
    "#             val = struct.unpack('I', fin.read(4))[0]\n",
    "            val = fin.seek(0, io.SEEK_END)\n",
    "            print(val)\n",
    "            return val\n",
    "    \n",
    "    def iterate_from(self, start_tok):\n",
    "        # Start by feeding from the cache, if possible.\n",
    "        if self._cache[0] <= start_tok < self._cache[1]:\n",
    "            for tok in self._cache[2][start_tok - self._cache[0] :]:\n",
    "                yield tok\n",
    "                start_tok += 1\n",
    "\n",
    "        # Decide where in the file we should start.  If `start` is in\n",
    "        # our mapping, then we can jump straight to the correct block;\n",
    "        # otherwise, start at the last block we've processed.\n",
    "        if start_tok < self._toknum[-1]:\n",
    "            block_index = bisect.bisect_right(self._toknum, start_tok) - 1\n",
    "            toknum = self._toknum[block_index]\n",
    "            filepos = self._filepos[block_index]\n",
    "        else:\n",
    "            block_index = len(self._toknum) - 1\n",
    "            toknum = self._toknum[-1]\n",
    "            filepos = self._filepos[-1]\n",
    "\n",
    "        # Open the stream, if it's not open already.\n",
    "        if self._stream is None:\n",
    "            self._open()\n",
    "\n",
    "        # If the file is empty, the while loop will never run.\n",
    "        # This *seems* to be all the state we need to set:\n",
    "        if self._eofpos == 0:\n",
    "            self._len = 0\n",
    "\n",
    "        # Each iteration through this loop, we read a single block\n",
    "        # from the stream.\n",
    "        while filepos < self._eofpos:\n",
    "            # Read the next block.\n",
    "            self._stream.seek(filepos)\n",
    "            self._current_toknum = toknum\n",
    "            self._current_blocknum = block_index\n",
    "            tokens = self.read_block(self._stream)\n",
    "            assert isinstance(tokens, (tuple, list, AbstractLazySequence)), (\n",
    "                'block reader %s() should return list or tuple.'\n",
    "                % self.read_block.__name__\n",
    "            )\n",
    "            num_toks = len(tokens)\n",
    "            new_filepos = self._stream.tell()\n",
    "            if new_filepos <= filepos:\n",
    "                print(new_filepos, filepos)\n",
    "            assert new_filepos > filepos, (\n",
    "                'block reader %s() should consume at least 1 byte (filepos=%d)'\n",
    "                % (self.read_block.__name__, filepos)\n",
    "            )\n",
    "\n",
    "            # Update our cache.\n",
    "            self._cache = (toknum, toknum + num_toks, list(tokens))\n",
    "\n",
    "            # Update our mapping.\n",
    "            assert toknum <= self._toknum[-1]\n",
    "            if num_toks > 0:\n",
    "                block_index += 1\n",
    "                if toknum == self._toknum[-1]:\n",
    "                    assert new_filepos > self._filepos[-1]  # monotonic!\n",
    "                    self._filepos.append(new_filepos)\n",
    "                    self._toknum.append(toknum + num_toks)\n",
    "                else:\n",
    "                    # Check for consistency:\n",
    "                    assert (\n",
    "                        new_filepos == self._filepos[block_index]\n",
    "                    ), 'inconsistent block reader (num chars read)'\n",
    "                    assert (\n",
    "                        toknum + num_toks == self._toknum[block_index]\n",
    "                    ), 'inconsistent block reader (num tokens returned)'\n",
    "\n",
    "            # If we reached the end of the file, then update self._len\n",
    "            if new_filepos == self._eofpos:\n",
    "                self._len = toknum + num_toks\n",
    "            # Generate the tokens in this block (but skip any tokens\n",
    "            # before start_tok).  Note that between yields, our state\n",
    "            # may be modified.\n",
    "            for tok in tokens[max(0, start_tok - toknum) :]:\n",
    "                yield tok\n",
    "            # If we're at the end of the file, then we're done.\n",
    "            assert new_filepos <= self._eofpos\n",
    "            if new_filepos == self._eofpos:\n",
    "                break\n",
    "            # Update our indices\n",
    "            toknum += num_toks\n",
    "            filepos = new_filepos\n",
    "\n",
    "        # If we reach this point, then we should know our length.\n",
    "        assert self._len is not None\n",
    "        # Enforce closing of stream once we reached end of file\n",
    "        # We should have reached EOF once we're out of the while loop.\n",
    "        self.close()\n",
    "\n",
    "class NewTwitterCorpusReader(TwitterCorpusReader):\n",
    "    \n",
    "#     CorpusView = StreamBackedCorpusView\n",
    "    \n",
    "    def __init__(\n",
    "        self, root, fileids=None, word_tokenizer=TweetTokenizer(), encoding='utf8'\n",
    "    ):\n",
    "        CategorizedCorpusReader.__init__(self, root, )\n",
    "        TwitterCorpusReader.__init__(self, root, fileids)\n",
    "        \n",
    "        if isinstance(root, string_types) and not isinstance(root, PathPointer):\n",
    "            m = re.match('(.*\\.gz)/?(.*)$|', root) #'(.*\\.zip)/?(.*\\.gz)/?(.*)$|'\n",
    "            zipfile, zipentry = m.groups()\n",
    "            print(zipfile, zipentry)\n",
    "            if zipfile:\n",
    "                root = ZipFilePathPointer(zipfile, zipentry)\n",
    "            else:\n",
    "                root = FileSystemPathPointer(root)\n",
    "                print(type(root))\n",
    "        elif not isinstance(root, PathPointer):\n",
    "            raise TypeError('CorpusReader: expected a string or a PathPointer')\n",
    "        \n",
    "        self._root = root\n",
    "    \n",
    "    def docs(self, fileids = None):\n",
    "        #Concat is a method to create a ConcattenatedStreamBackedCorpusView\n",
    "        return concat(\n",
    "            [\n",
    "                self.CorpusView(path, block_reader = self._read_tweets, encoding=enc)\n",
    "                for (path, enc, fileid) in self.abspaths(fileids, True, True)\n",
    "            ]\n",
    "        )\n",
    "    \n",
    "    def abspaths(self, fileids=None, include_encoding=False, include_fileid=False):\n",
    "        if fileids is None:\n",
    "            fileids = self._fileids\n",
    "        elif isinstance(fileids, string_types):\n",
    "            fileids = [fileids]\n",
    "\n",
    "        paths = [GzipFileSystemPathPointer(self._root.join(f)) for f in fileids]\n",
    "\n",
    "        if include_encoding and include_fileid:\n",
    "            return list(zip(paths, [self.encoding(f) for f in fileids], fileids))\n",
    "        elif include_fileid:\n",
    "            return list(zip(paths, fileids))\n",
    "        elif include_encoding:\n",
    "            return list(zip(paths, [self.encoding(f) for f in fileids]))\n",
    "        else:\n",
    "            return paths\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None None\n",
      "<class 'nltk.data.FileSystemPathPointer'>\n"
     ]
    }
   ],
   "source": [
    "corpus = NewTwitterCorpusReader('Twitter-Data/', ['Manhattan-2019-11-07-000.json.gz'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#At the moment, it is also possible to pass it a root directory that is itself zipped\n",
    "#As in the root directory is a sipped folder (holding the zipped files)\n",
    "#Step 2:\n",
    "# We need to work out a way to... integrate the current state of our dataset to the actual\n",
    "# already built pipeline (which is built for html documents, and needs redesigning to additionally\n",
    "# take into consideration location etc. and extract fulltext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = r'Twitter-Data/Data/'\n",
    "DOC_PATTERN = r'Manhattan.*\\.json\\.gz$'\n",
    "PKL_PATTERN = r'^Manhattan\\.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = re.match(DOC_PATTERN,'Manhattan-2019-11-07-000.json.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Manhattan-2019-11-07-000.json.gz'"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-urban",
   "language": "python",
   "name": "venv-urban"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
