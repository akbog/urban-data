{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DTM: Twitter Corpus Reader, Preprocessor, & Sample Analysis\n",
    "\n",
    "### By. Alexander Bogdanowicz\n",
    "\n",
    "#### This notebook runs through newly implemented modules and their interfaces, these include:\n",
    "    1. NewTwitterReader\n",
    "    2. NewTwitterPreprocessor\n",
    "    3. NewTwitterCorpusLoader\n",
    "    4. NewTwitterCorpusView\n",
    "    5. NewTwitterCorpusTranformer\n",
    "    \n",
    "#### These Modules can be found here: https://github.com/akbog/urban-data\n",
    "#### This notebook is also designed to run through the following steps:\n",
    "    1. Establishing a local version of the github repository \"urban-data\" (Part 0)\n",
    "    2. Structuring the file-system in a baleen structure for easy readibility by the CorpusReaders\n",
    "    3. Running a sample LDA Model\n",
    "    \n",
    "#### Please see Part 0. at the bottom of this notebook for directions on how to initialize a local github repository"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0. Before you get started: Initializing Your Local Github Repository\n",
    "\n",
    "##### Before attempting this, please make sure you have a valid Github account and have git installed on your machine. \n",
    "\n",
    "##### If not, see (Installing Git on Linux, Mac, Windows) https://gist.github.com/derhuerst/1b15ff4652a867391f03\n",
    "\n",
    "##### To clone the github repository, navigate to the folder you would like to clone the repository into and type the following after the \">\":\n",
    "\n",
    "    > git clone https://github.com/akbog/urban-data.git\n",
    "\n",
    "##### You should find that the files from the github repository are now in your local file system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Importing Neccesary Modules\n",
    "from Modules.NewTwitterReader import GzipStreamBackedCorpusView, NewTwitterCorpusReader, NewTwitterPickledCorpusReader\n",
    "from Modules.NewTwitterPreprocessor import Preprocessor\n",
    "from Modules.NewPickleCorpusView import PickleCorpusView\n",
    "from Modules.NewTwitterCorpusLoader import NewTwitterCorpusLoader\n",
    "from Modules.NewTwitterTransformer import TextNormalizer, GensimTfidfVectorizer, GensimTopicModels\n",
    "from sklearn.pipeline import Pipeline\n",
    "from gensim.sklearn_api import lsimodel, ldamodel\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. Preprocessing Stage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The NewTwitterCorpusReader assumes that collections of tweets are stored in .gz files (to conserve disk space) (this is often the format that api's may export tweets).\n",
    "\n",
    "##### We store our data *outside* the urban-data local github repository, as the data tends to be large and is not suited for github's code repository. In this example, our data will be stored in the following hierarchical structure:\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; -> urban-data </br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; -> Twitter-Data </br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -> Category1 (2019-01-07) </br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -> Raw Gzipped JSON File (Manhattan-2019-11-07-000.json.gz) </br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -> Category2 </br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -> Category3 </br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -> Category4 </br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -> Raw Gzipped JSON File </br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; -> Twitter-Data-Pkl </br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -> Category1 (2019-01-07) </br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -> Raw Gzipped JSON File (Manhattan-2019-11-07-000.pickle) </br>\n",
    "\n",
    "\n",
    "##### For the purposes of this notebook, our categories are divided into separate dates, each date containing one JSON Gzipped File.\n",
    "\n",
    "##### We also see a Twitter-Data-Pkl Folder, which will contain the Preprocessed (Tokenized) Tweets, pickled in their Python Readable Formats (List of Dictionaries of Tweets)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The following are Regex Strings designed to match general forms of our files\n",
    "DOC_PATTERN = r'[0-2][0-9][0-9][0-9]-[0-3][0-9]-[0-1][0-9]/Manhattan.*\\.json\\.gz$' #Document Regex\n",
    "root = r\"../Twitter-Data\" #Root Data Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiating our Corpus Reader Object with a root directory\n",
    "pre_corpus = NewTwitterCorpusReader(root = root)\n",
    "print(\"Sample Tweet Structure\")\n",
    "print(pre_corpus.docs()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The below stage may take some time as the Preprocessor tokenizes each tweet and writes them to the target directory in pickle format\n",
    "\n",
    "##### Note. The file-size will naturally increase as pickling is a less efficient compressions (thus more easily readable) than Gzipped files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing the Preprocessor\n",
    "target = r\"../Twitter-Data-Pkl\" #Specifying target folder for preprocessed corpus\n",
    "preprocess = Preprocessor(corpus = pre_corpus, target = target) #Initializing \n",
    "CAT_PATTERN = r'[0-2][0-9][0-9][0-9]-[0-3][0-9]-[0-1][0-9]' #Category Regex\n",
    "\n",
    "#Calling Transformation which Tokenizes the Dataset and Pickles the result to the target directory\n",
    "docs = preprocess.transform(categories = CAT_PATTERN) \n",
    "print(\"Done: \", len(list(docs))) #Must call as docs is simply a generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. Reading Pickled Corpus & Initial Test Analysis\n",
    "\n",
    "##### In this section, we will run a simple analysis that demonstrates how well we have been able to abstract each step of the modeling process. We will run a simple Gensim Based Latent Dirichlet Allocation Model on our tweets to try and extract key models.\n",
    "\n",
    "##### As this can take some time, we will only perform the analysis for the day of November 7th, 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specifying root directory of Pickled Tweets\n",
    "pkl_root = r\"../Twitter-Data-Pkl\"\n",
    "corpus = NewTwitterPickledCorpusReader(pkl_root) #Initializing Pickled Corpus Reader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The following steps occur when instantiating and fitting a GensimTopicModels Object\n",
    "    1. Text Normalization (consists of stopword removal and lemmatization)\n",
    "    2. Text Vectorization (In this sample, TF-IDF Matrix measureing term frequency/tweet and corpus)\n",
    "    3. Text Model (Gensim LDA Model generating 10 Topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CAT_PATTERN = r'[0-2][0-9][0-9][0-9]-[0-3][0-9]-[0-1]7' #Category Regex\n",
    "#### docs object for use in fitting the model (consists of our tweets in the specified category)\n",
    "docs = [\n",
    "    tweet for tweet in corpus.tweets(categories = CAT_PATTERN)\n",
    "]\n",
    "gensim_lda = GensimTopicModels(n_topics = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting the model (This may take some time)\n",
    "gensim_lda.fit(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3. Sample Visualization with pyLDAvis\n",
    "\n",
    "#### pyLDAvis is a highly interactive visualization of the output of an LDA Topic Model\n",
    "\n",
    "##### Note. This code is directly sampled from Applied Text Analytics with Python By. Bengfort, Bilbro & Ojeda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting the LDA Model\n",
    "lda = gensim_lda.model.named_steps['model'].gensim_model\n",
    "#extracting the corpus vectors\n",
    "corpus = [\n",
    "    gensim_lda.model.named_steps['vect'].lexicon.doc2bow(doc)\n",
    "    for doc in gensim_lda.model.named_steps['norm'].transform(docs)\n",
    "]\n",
    "#extracing the corresponding lexicon\n",
    "lexicon = gensim_lda.model.named_steps['vect'].lexicon\n",
    "\n",
    "#creating formatted data for pyLDAvis (and correcting)\n",
    "data = pyLDAvis.gensim.prepare(lda, corpus, lexicon)\n",
    "data[0][\"x\"] = np.real(data[0][\"x\"])\n",
    "data[0][\"y\"] = np.real(data[0][\"y\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.display(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-urban",
   "language": "python",
   "name": "venv-urban"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
